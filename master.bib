
@article{altes1979_TargPosi,
  title = {Target Position Estimation in Radar and Sonar, and Generalized Ambiguity Analysis for Maximum Likelihood Parameter Estimation},
  author = {Altes, R.A.},
  year = {1979},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {67},
  number = {6},
  pages = {920--930},
  issn = {1558-2256},
  doi = {10/dxdc7v},
  abstract = {Target position estimation in radar and sonar means joint estimation of range and angle in the presence of noise and clutter. The global behavior of a maximum likelihood (ML) position estimator, and the clutter suppression capability of the system, can be written in terms of a range-angle ambiguity function. This function depends upon signal waveform and array configuration, i.e., upon both temporal and spatial characteristics of the system. Ambiguity and variance bound analysis indicates that system bandwidth can often be traded for array size, and direction-dependent signals can be used to obtain better angle resolution without increasing the size of the array. Wide-band direction-dependent signals (temporal diversity) can be traded for large real or synthetic arrays (spatial diversity). This tradeoff is apparently exploited by some animal echolocation systems. The above insights are obtained mostly from the properties of the range-angle ambiguity function. In general, an appropriate ambiguity function should be very useful for the design and evaluation of any ML parameter estimator. System identification methods and radio navigation systems, for example, can be optimized by minimizing the volume of a multiparameter ambiguity function.},
  keywords = {Analysis of variance,Bandwidth,Maximum likelihood estimation,Parameter estimation,Radar clutter,Signal analysis,Signal resolution,Sonar,Spatial resolution,Wideband}
}

@article{anderson2015_SimuSide,
  title = {Simulating {{Side Scan Sonar}} as a {{MOOS}} App Using {{NVIDIA OptiX}}},
  author = {Anderson, Trevor},
  year = {2015},
  pages = {24},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{bava-de-camargo2015_UseSide,
  title = {The Use of Side Scan Sonar in {{Brazilian Underwater Archaeology}}},
  booktitle = {2015 {{IEEE}}/{{OES Acoustics}} in {{Underwater Geosciences Symposium}} ({{RIO Acoustics}})},
  author = {{Bava-de-Camargo}, Paulo Fernando},
  year = {2015},
  pages = {1--7},
  publisher = {{IEEE}},
  doi = {10/gpb94j}
}

@misc{beaumont2021_ForwSona,
  title = {Forward-Looking Sonars},
  author = {Beaumont, Dick},
  year = {2021},
  month = mar,
  journal = {Ocean Sailor Magazine},
  url = {https://oceansailormagazine.com/forward-looking-sonars/},
  urldate = {2022-01-28},
  abstract = {Case History By Dick Beaumont Fifteen years ago, on passage from Kota Kinabalu, Malaysia to Phuket Thailand, in Moonshadow, my Tayana 58, I motored, at dusk, into an anchorage called Port Klang, in northern Malaysia, it was very aptly named as we later found out. The anchorage there was clearly marked on my Navionics chart, [\ldots ]},
  langid = {american}
}

@article{benjaminOverMOOS,
  title = {An {{Overview}} of {{MOOS-IvP}} and a {{Users Guide}} to the {{IvP Helm}} - {{Release}} 19.8},
  author = {Benjamin, Michael R and Schmidt, Henrik and Newman, Paul},
  pages = {372},
  abstract = {This document describes the IvP Helm - an Open Source behavior-based autonomy application for unmanned vehicles. IvP is short for interval programming - a technique for representing and solving multi-objective optimizations problems. Behaviors in the IvP Helm are reconciled using multi-objective optimization when in competition with each other for influence of the vehicle. The IvP Helm is written as a MOOS application where MOOS is a set of Open Source publish-subscribe autonomy middleware tools. This document describes the configuration and use of the IvP Helm, provides examples of simple missions and information on how to download and build the software from the MOOS-IvP server at www.moos-ivp.org.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{blueroboticsUndeUsin,
  title = {Understanding and {{Using Scanning Sonars}}},
  author = {{Blue Robotics}},
  journal = {Blue Robotics},
  url = {https://bluerobotics.com/learn/understanding-and-using-scanning-sonars/},
  urldate = {2022-02-03},
  abstract = {Mechanical scanning sonars are one of the most popular types of ROV sonar. This guide explains how they work, how to understand the sonar images to produce, and how to use them operationally.},
  langid = {american}
}

@article{burguera2012_UspIPerf,
  title = {The {{UspIC}}: {{Performing Scan Matching Localization Using}} an {{Imaging Sonar}}},
  shorttitle = {The {{UspIC}}},
  author = {Burguera, Antoni and Cid, Yolanda and Oliver, Gabriel},
  year = {2012},
  month = dec,
  journal = {Sensors (Basel, Switzerland)},
  volume = {12},
  pages = {7855--85},
  doi = {10/gcfmbk},
  abstract = {This paper presents a novel approach to localize an underwater mobile robot based on scan matching using a Mechanically Scanned Imaging Sonar (MSIS). When used to perform scan matching, this sensor presents some problems such as significant uncertainty in the measurements or large scan times, which lead to a motion induced distortion. This paper presents the uspIC, which deals with these problems by adopting a probabilistic scan matching strategy and by defining a method to strongly alleviate the motion induced distortion. Experimental results evaluating our approach and comparing it to previously existing methods are provided.}
}

@article{burguera2016_HighUnde,
  title = {High-Resolution Underwater Mapping Using Side-Scan Sonar},
  author = {Burguera, Antoni and Oliver, Gabriel},
  year = {2016},
  journal = {PloS one},
  volume = {11},
  number = {1},
  pages = {e0146396},
  publisher = {{Public Library of Science San Francisco, CA USA}},
  doi = {10/f8qbpd}
}

@article{burguera2020_OnLiMult,
  title = {On-{{Line Multi-Class Segmentation}} of {{Side-Scan Sonar Imagery Using}} an {{Autonomous Underwater Vehicle}}},
  author = {Burguera, Antoni and {Bonin-Font}, Francisco},
  year = {2020},
  journal = {Journal of Marine Science and Engineering},
  volume = {8},
  number = {8},
  pages = {557},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10/gpbh84}
}

@article{carter1981_TimeDela,
  title = {Time Delay Estimation for Passive Sonar Signal Processing},
  author = {Carter, G.},
  year = {1981},
  month = jun,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {29},
  number = {3},
  pages = {463--470},
  issn = {0096-3518},
  doi = {10/d2txr2},
  abstract = {An overview of applied research in passive sonar signal processing estimation techniques for naval systems is presented. The naval problem that motivates time delay estimation is the source state estimation problem. A discussion of this problem in terms of estimating the position and velocity of a moving acoustic source is presented. Optimum bearing and range estimators are presented for the planar problem and related to the optimum time delay vector estimator. Suboptimum realizations are considered together with the effects of source motion and receiver positional uncertainty.},
  keywords = {Acoustic sensors,Acoustic signal processing,Biomedical signal processing,Biosensors,Delay effects,Delay estimation,Oceans,Signal processing,Sonar applications,Sonar detection}
}

@misc{ceehydrosystemsSingBeam,
  title = {Single {{Beam Echo Sounders}}: {{Ultra-rugged}} Compact Single Beam Echo Sounder Survey Systems},
  author = {{CEE HydroSystems}},
  url = {https://ceehydrosystems.com/products/single-beam-echo-sounders/},
  urldate = {2022-01-28},
  langid = {australian}
}

@inproceedings{cerqueira2016_CustShad,
  title = {Custom Shader and 3d Rendering for Computationally Efficient Sonar Simulation},
  booktitle = {29th {{Conference}} on {{Graphics}}, {{Patterns}} and {{Images-SIBGRAPI}}},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Neves, Gustavo and Oliveira, Luciano and Joyeux, Sylvain and Albiez, Jan and Center, Robotics Innovation},
  year = {2016},
  keywords = {⛔ No DOI found}
}

@article{cerqueira2020_RastRayt,
  title = {A Rasterized Ray-Tracer Pipeline for Real-Time, Multi-Device Sonar Simulation},
  author = {Cerqueira, R{\^o}mulo and Trocoli, Tiago and Albiez, Jan and Oliveira, Luciano},
  year = {2020},
  month = sep,
  journal = {Graphical Models},
  volume = {111},
  pages = {101086},
  issn = {1524-0703},
  doi = {10/gpcbh7},
  url = {https://www.sciencedirect.com/science/article/pii/S1524070320300278},
  urldate = {2022-02-03},
  abstract = {Simulating sonar devices requires modeling underwater acoustics, simultaneously rendering time-efficient data. Existing methods focus on a basic implementation of one sonar type, where most of the sound properties are disregarded. In this context, this work presents a multi-device sonar simulator capable of processing an underwater scene by a hybrid pipeline on GPU: Rasterization computes the primary intersections, while only the reflective areas are ray-traced. Our proposed system launches a few rays when compared to a full ray-based method, achieving a significant performance gain without quality loss in the final rendering. Resulting reflections are then characterized as two sonar parameters: Echo intensity and pulse distance. Underwater acoustic features, such as speckle noise, transmission loss, reverberation and material properties of observable objects are also computed in the final acoustic image. Visual and numerical performance assessments demonstrated the effectiveness of the proposed simulator to render underwater scenes in comparison to real sonar devices.},
  langid = {english},
  keywords = {Acoustic images,Imaging sonar simulation,Multipath propagation,Rasterization,Ray-tracing,Underwater robotics}
}

@article{cerqueiraCustShad,
  title = {Custom {{Shader}} and {{3D Rendering}} for Computationally Efficient {{Sonar Simulation}}},
  author = {Cerqueira, Romulo and Trocoli, Tiago and Neves, Gustavo and Oliveira, Luciano and Joyeux, Sylvain and Albiez, Jan},
  pages = {4},
  abstract = {This paper introduces a novel method for simulating underwater sonar sensors by vertex and fragment processing. The virtual scenario used is composed of the integration between the Gazebo simulator and the Robot Construction Kit (ROCK) framework. A 3-channel matrix with depth and intensity buffers and angular distortion values is extracted from OpenSceneGraph 3D scene frames by shader rendering, and subsequently fused and processed to generate the synthetic sonar data. To export and display simulation resources, this approach was written in C++ as ROCK packages. The method is evaluated on two use cases: the virtual acoustic images from a mechanical scanning sonar and forward-looking sonar simulations.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{chappleAutoDete,
  title = {Automated {{Detection}} and {{Classification}} in {{High-resolution Sonar Imagery}} for {{Autonomous Underwater Vehicle Operations}}},
  author = {Chapple, Philip},
  pages = {37},
  abstract = {Autonomous Underwater Vehicles (AUVs) are increasingly being used by military forces to acquire high-resolution sonar imagery, in order to detect mines and other objects of interest on the seabed. Automatic detection and classification teclmiques are being developed for several reasons: to provide reliable and consistent detection of objects on the seabed; to free human analysts from time-consuming and tedious detection tasks; and to enable autonomous in-field decision-making based on observations of mines and other objects. This document reviews progress in the development of automated detection and classification teclmiques for side-looking sonars mounted on AUVs. Whilst the teclmiques have not yet reached maturity, considerable progress has been made in both unsupervised and supervised (trained) algoritluns for feature detection and classification. In some cases, the perfonnance and reliability of automated detection systems exceed those of human operators.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{choi2021_PhysMode,
  title = {Physics-{{Based Modelling}} and {{Simulation}} of {{Multibeam Echosounder Perception}} for {{Autonomous Underwater Manipulation}}},
  author = {Choi, Woen-Sug and Olson, Derek R. and Davis, Duane and Zhang, Mabel and Racson, Andy and Bingham, Brian and McCarrin, Michael and Vogt, Carson and Herman, Jessica},
  year = {2021},
  journal = {Frontiers in Robotics and AI},
  volume = {8},
  issn = {2296-9144},
  doi = {10/gpcdgq},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2021.706646},
  urldate = {2022-02-03},
  abstract = {One of the key distinguishing aspects of underwater manipulation tasks is the perception challenges of the ocean environment, including turbidity, backscatter, and lighting effects. Consequently, underwater perception often relies on sonar-based measurements to estimate the vehicle's state and surroundings, either standalone or in concert with other sensing modalities, to support the perception necessary to plan and control manipulation tasks. Simulation of the multibeam echosounder, while not a substitute for in-water testing, is a critical capability for developing manipulation strategies in the complex and variable ocean environment. Although several approaches exist in the literature to simulate synthetic sonar images, the methods in the robotics community typically use image processing and video rendering software to comply with real-time execution requirements. In addition to a lack of physics-based interaction model between sound and the scene of interest, several basic properties are absent in these rendered sonar images\textendash notably the coherent imaging system and coherent speckle that cause distortion of the object geometry in the sonar image. To address this deficiency, we present a physics-based multibeam echosounder simulation method to capture these fundamental aspects of sonar perception. A point-based scattering model is implemented to calculate the acoustic interaction between the target and the environment. This is a simplified representation of target scattering but can produce realistic coherent image speckle and the correct point spread function. The results demonstrate that this multibeam echosounder simulator generates qualitatively realistic images with high efficiency to provide the sonar image and the physical time series signal data. This synthetic sonar data is a key enabler for developing, testing, and evaluating autonomous underwater manipulation strategies that use sonar as a component of perception.}
}

@misc{ChooRigh,
  title = {Choosing the {{Right Sonar Imaging System}} for {{Autonomous Underwater Vehicles}}},
  url = {https://www.hydro-international.com/content/article/choosing-the-right-sonar-imaging-system-for-autonomous-underwater-vehicles},
  urldate = {2022-01-27},
  abstract = {Underwater vehicle operators have to make a choice when deciding on a sonar imaging system to integrate into their systems. The decision could result...},
  langid = {english}
}

@article{cobra1992_GeomDist,
  title = {Geometric Distortions in Side-Scan Sonar Images: {{A}} Procedure for Their Estimation and Correction},
  shorttitle = {Geometric Distortions in Side-Scan Sonar Images},
  author = {Cobra, Daniel T. and Oppenheim, Alan V. and Jaffe, Jules S.},
  year = {1992},
  journal = {IEEE Journal of oceanic engineering},
  volume = {17},
  number = {3},
  pages = {252--268},
  publisher = {{IEEE}},
  doi = {10/cb53n7}
}

@misc{deepvisionSonaSoft,
  title = {Sonar {{Software}} | {{DeepVision}}},
  author = {{DeepVision}},
  url = {https://deepvision.se/download/sonar-software/},
  urldate = {2022-01-31},
  langid = {american}
}

@inproceedings{demarco2011_ImplROS,
  title = {An Implementation of {{ROS}} on the {{Yellowfin}} Autonomous Underwater Vehicle ({{AUV}})},
  booktitle = {{{OCEANS}}'11 {{MTS}}/{{IEEE KONA}}},
  author = {DeMarco, Kevin and West, Michael E. and Collins, Thomas R.},
  year = {2011},
  pages = {1--7},
  publisher = {{IEEE}},
  doi = {10/gpbpqq}
}

@misc{demarco2021_Moos,
  title = {Moos-Ros-Bridge},
  author = {DeMarco, Kevin},
  year = {2021},
  month = aug,
  url = {https://github.com/SyllogismRXS/moos-ros-bridge},
  urldate = {2022-02-01},
  abstract = {A communication bridge between the MOOS and ROS publish-and-subscribe middleware systems.},
  copyright = {MIT}
}

@article{deseixas2011_PrepPass,
  title = {Preprocessing Passive Sonar Signals for Neural Classification},
  author = {De Seixas, J. M. and De Moura, N. N.},
  year = {2011},
  journal = {IET radar, sonar \& navigation},
  volume = {5},
  number = {6},
  pages = {605--612},
  publisher = {{IET}},
  doi = {10/dbm6dm}
}

@article{dushaw1993_EquaSpee,
  title = {On Equations for the Speed of Sound in Seawater},
  author = {Dushaw, Brian D. and Worcester, Peter F. and Cornuelle, Bruce D. and Howe, Bruce M.},
  year = {1993},
  journal = {The Journal of the Acoustical Society of America},
  volume = {93},
  number = {1},
  pages = {255--275},
  publisher = {{Acoustical Society of America}},
  doi = {10/dgkwvh}
}

@article{edgar2011_IntrSynt,
  title = {Introduction to {{Synthetic Aperture Sonar}}},
  author = {Edgar, Roy},
  year = {2011},
  journal = {Sonar Systems},
  pages = {1--11},
  keywords = {⛔ No DOI found}
}

@techreport{edgetech2006_DescEdge,
  title = {Description of the {{Edgetech}} (.Jsf) {{File Format}}},
  author = {{EdgeTech}},
  year = {2006},
  url = {https://www3.mbari.org/data/mbsystem/formatdoc/JSF_format_Rev17.pdf},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{field-robotics-labMultForw,
  title = {Multibeam {{Forward Looking Sonar}} {$\cdot$} {{Field-Robotics-Lab}}/Dave {{Wiki}}},
  author = {{Field-Robotics-Lab}},
  journal = {GitHub},
  url = {https://github.com/Field-Robotics-Lab/dave},
  urldate = {2022-02-03},
  langid = {english}
}

@misc{geoscienceaustralia2014_MultSona,
  title = {Multibeam Sonar},
  author = {{Geoscience Australia}},
  year = {2014},
  month = may,
  url = {https://www.ga.gov.au/scientific-topics/marine/survey-techniques/sonar/multibeam-sonar},
  urldate = {2022-01-28},
  abstract = {Geoscience Australia is the national public sector geoscience organisation. Its mission is to be the trusted source of information on Australia's geology and geography to inform government, industry and community decision-making. The work of Geoscience Australia covers the Australian landmass, marine jurisdiction and territories in Antarctica.},
  copyright = {http://creativecommons.org/licenses/by/2.5/au/},
  langid = {english}
}

@article{hahner2019_SemaUnde,
  title = {Semantic {{Understanding}} of {{Foggy Scenes}} with {{Purely Synthetic Data}}},
  author = {Hahner, Martin and Dai, Dengxin and Sakaridis, Christos and Zaech, Jan-Nico and Van Gool, Luc},
  year = {2019},
  month = oct,
  journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  eprint = {1910.03997},
  eprinttype = {arxiv},
  pages = {3675--3681},
  doi = {10/gn6d93},
  url = {http://arxiv.org/abs/1910.03997},
  urldate = {2022-01-18},
  abstract = {This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen real-world foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{hayes2009_SyntAper,
  title = {Synthetic {{Aperture Sonar}}: {{A Review}} of {{Current Status}}},
  shorttitle = {Synthetic {{Aperture Sonar}}},
  author = {Hayes, Michael P. and Gough, Peter T.},
  year = {2009},
  month = jul,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {34},
  number = {3},
  pages = {207--224},
  issn = {1558-1691},
  doi = {10/bkjgc3},
  abstract = {This is a review paper that surveys past work in, and the recent status of, active synthetic aperture sonar (SAS). It covers the early historical development of SAS with its provenance in synthetic aperture radar (SAR) and flows through into what work has been published in the open literature up to early 2007. The list of references is sufficiently complete to include most past and recent SAS publications in the open refereed literature.},
  keywords = {Costs,Energy resolution,Image reconstruction,Image resolution,Oceanographic techniques,Optical transmitters,Review,SAS,Sea floor,sonar,Sonar equipment,synthetic aperture,Synthetic aperture radar,Synthetic aperture sonar}
}

@article{hayes2009_SyntAper-1,
  title = {Synthetic Aperture Sonar: A Review of Current Status},
  shorttitle = {Synthetic Aperture Sonar},
  author = {Hayes, Michael P. and Gough, Peter T.},
  year = {2009},
  journal = {IEEE journal of oceanic engineering},
  volume = {34},
  number = {3},
  pages = {207--224},
  publisher = {{IEEE}},
  doi = {10/bkjgc3}
}

@inproceedings{howell2003_PassSona,
  title = {Passive Sonar Recognition and Analysis Using Hybrid Neural Networks},
  booktitle = {Oceans 2003. {{Celebrating}} the {{Past}} ... {{Teaming Toward}} the {{Future}} ({{IEEE Cat}}. {{No}}.{{03CH37492}})},
  author = {Howell, B.P. and Wood, S.},
  year = {2003},
  month = sep,
  volume = {4},
  pages = {1917-1924 Vol.4},
  doi = {10/bgw7fn},
  abstract = {The detection, classification, and recognition of underwater acoustic features have always been of the highest importance for scientific, fisheries, and defense interests. Recent efforts in improved passive sonar techniques have only emphasized this interest. In this paper, the authors describe the use of novel, hybrid neural approaches using both unsupervised and supervised network topologies. Results are presented which demonstrate the ability of the network to classify biological, man made, and geological sources. Also included are capabilities of the networks to attack the more difficult problems of identifying the complex vocalizations of several fish and marine mammalian species. Basic structure, processor requirements, training and operational methodologies are described as well as application to autonomous observation and vehicle platforms.},
  keywords = {Acoustic signal detection,Aquaculture,Geology,Marine animals,Network topology,Neural networks,Remotely operated vehicles,Sonar,Underwater acoustics,Underwater tracking}
}

@inproceedings{hurtos2013_AutoDete,
  title = {Automatic Detection of Underwater Chain Links Using a Forward-Looking Sonar},
  booktitle = {2013 {{MTS}}/{{IEEE OCEANS}} - {{Bergen}}},
  author = {Hurtos, Natalia and Palomeras, Narcis and Nagappa, Sharad and Salvi, Joaquim},
  year = {2013},
  month = jun,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Bergen}},
  doi = {10/gn9xgd},
  url = {http://ieeexplore.ieee.org/document/6608106/},
  urldate = {2022-01-28},
  abstract = {Underwater chain cleaning and inspection tasks are costly and time consuming operations that must be performed periodically to guarantee the safety of the moorings. We propose a framework towards an efficient and costeffective solution by using an autonomous underwater vehicle equipped with a forward-looking sonar. As a first step, we tackle the problem of individual chain link detection from the challenging forward-looking sonar data. To cope with occlusions and intensity variations due to viewpoint changes, the recognition problem is addressed as local pattern matching of the different link parts. We exploit the high frame-rate of the sonar to improve, by registration, the signal-to-noise ratio of the individual sonar frames and to cluster the local detections over time to increase robustness. Experiments with sonar images of a real chain are reported, showing a high percentage of correct link detections with good accuracy while potentially keeping real-time capabilities.},
  isbn = {978-1-4799-0001-5 978-1-4799-0000-8},
  langid = {english}
}

@inproceedings{hurtos2013_AutoDete-1,
  title = {Automatic Detection of Underwater Chain Links Using a Forward-Looking Sonar},
  booktitle = {2013 {{MTS}}/{{IEEE OCEANS-Bergen}}},
  author = {Hurt{\'o}s, Natalia and Palomeras, Narc{\'i}s and Nagappa, Sharad and Salvi, Joaquim},
  year = {2013},
  pages = {1--7},
  publisher = {{IEEE}},
  doi = {10/gn9xgd}
}

@inproceedings{hurtos2013_AutoDete-2,
  title = {Automatic Detection of Underwater Chain Links Using a Forward-Looking Sonar},
  booktitle = {2013 {{MTS}}/{{IEEE OCEANS}} - {{Bergen}}},
  author = {Hurtos, Natalia and Palomeras, Narcis and Nagappa, Sharad and Salvi, Joaquim},
  year = {2013},
  month = jun,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Bergen}},
  doi = {10/gn9xgd},
  url = {http://ieeexplore.ieee.org/document/6608106/},
  urldate = {2022-02-03},
  abstract = {Underwater chain cleaning and inspection tasks are costly and time consuming operations that must be performed periodically to guarantee the safety of the moorings. We propose a framework towards an efficient and costeffective solution by using an autonomous underwater vehicle equipped with a forward-looking sonar. As a first step, we tackle the problem of individual chain link detection from the challenging forward-looking sonar data. To cope with occlusions and intensity variations due to viewpoint changes, the recognition problem is addressed as local pattern matching of the different link parts. We exploit the high frame-rate of the sonar to improve, by registration, the signal-to-noise ratio of the individual sonar frames and to cluster the local detections over time to increase robustness. Experiments with sonar images of a real chain are reported, showing a high percentage of correct link detections with good accuracy while potentially keeping real-time capabilities.},
  isbn = {978-1-4799-0001-5 978-1-4799-0000-8},
  langid = {english}
}

@inproceedings{hurtos2013_AutoDete-3,
  title = {Automatic Detection of Underwater Chain Links Using a Forward-Looking Sonar},
  booktitle = {2013 {{MTS}}/{{IEEE OCEANS}} - {{Bergen}}},
  author = {Hurtos, Natalia and Palomeras, Narcis and Nagappa, Sharad and Salvi, Joaquim},
  year = {2013},
  month = jun,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Bergen}},
  doi = {10/gn9xgd},
  url = {http://ieeexplore.ieee.org/document/6608106/},
  urldate = {2022-02-03},
  abstract = {Underwater chain cleaning and inspection tasks are costly and time consuming operations that must be performed periodically to guarantee the safety of the moorings. We propose a framework towards an efficient and costeffective solution by using an autonomous underwater vehicle equipped with a forward-looking sonar. As a first step, we tackle the problem of individual chain link detection from the challenging forward-looking sonar data. To cope with occlusions and intensity variations due to viewpoint changes, the recognition problem is addressed as local pattern matching of the different link parts. We exploit the high frame-rate of the sonar to improve, by registration, the signal-to-noise ratio of the individual sonar frames and to cluster the local detections over time to increase robustness. Experiments with sonar images of a real chain are reported, showing a high percentage of correct link detections with good accuracy while potentially keeping real-time capabilities.},
  isbn = {978-1-4799-0001-5 978-1-4799-0000-8},
  langid = {english}
}

@article{johnson-roberson2017_DrivMatr,
  title = {Driving in the {{Matrix}}: {{Can Virtual Worlds Replace Human-Generated Annotations}} for {{Real World Tasks}}?},
  shorttitle = {Driving in the {{Matrix}}},
  author = {{Johnson-Roberson}, Matthew and Barto, Charles and Mehta, Rounak and Sridhar, Sharath Nittur and Rosaen, Karl and Vasudevan, Ram},
  year = {2017},
  month = feb,
  journal = {arXiv:1610.01983 [cs]},
  eprint = {1610.01983},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1610.01983},
  urldate = {2022-01-18},
  abstract = {Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have relied upon massive amounts of human annotated training data. This time consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for the training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor-based classification problems like those that appear in self-driving cars. The source code and data to train and validate the networks described in this paper are made available for researchers.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@article{johnson1990_GeolInte,
  title = {The Geological Interpretation of Side-Scan Sonar},
  author = {Johnson, H. Paul and Helferty, Maryann},
  year = {1990},
  journal = {Reviews of Geophysics},
  volume = {28},
  number = {4},
  pages = {357--380},
  publisher = {{Wiley Online Library}},
  doi = {10/fsmfdt}
}

@article{karimanzira2020_ObjeDete,
  title = {Object Detection in Sonar Images},
  author = {Karimanzira, Divas and Renkewitz, Helge and Shea, David and Albiez, Jan},
  year = {2020},
  journal = {Electronics},
  volume = {9},
  number = {7},
  pages = {1180},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10/gpcbkn}
}

@incollection{kuperman2014_UndeAcou,
  title = {Underwater {{Acoustics}}},
  booktitle = {Springer {{Handbook}} of {{Acoustics}}},
  author = {Kuperman, William A. and Roux, Philippe},
  editor = {Rossing, Thomas D.},
  year = {2014},
  series = {Springer {{Handbooks}}},
  pages = {157--212},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-0755-7_5},
  url = {https://doi.org/10.1007/978-1-4939-0755-7_5},
  urldate = {2022-01-27},
  abstract = {It is well established that sound waves, compared to electromagnetic waves, propagate long distances in the ocean. Hence, in the ocean as opposed to air or a vacuum, one uses sound navigation and ranging (SONAR) instead of radar, acoustic communication instead of radio, and acoustic imaging and tomography instead of microwave or optical imaging or X-ray tomography. Underwater acoustics is the science of sound in water (most commonly in the ocean) and encompasses not only the study of sound propagation, but also the masking of sound signals by interfering phenomenon and signal processing for extracting these signals from interference. This chapter we will present the basics physics of ocean acoustics and then discuss applications.},
  isbn = {978-1-4939-0755-7},
  langid = {english}
}

@inproceedings{kwak2015_DeveAcou,
  title = {Development of Acoustic Camera-Imaging Simulator Based on Novel Model},
  booktitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Environment}} and {{Electrical Engineering}} ({{EEEIC}})},
  author = {Kwak, Seungchul and Ji, Yonghoon and Yamashita, Atsushi and Asama, Hajime},
  year = {2015},
  month = jun,
  pages = {1719--1724},
  doi = {10/gpcdgw},
  abstract = {Acquisition of accurate image information from underwater environments is essential to operate underwater tasks such as maintenance, inspection, trajectory estimation, target recognition and SLAM (simultaneous localization and mapping). In this respect, an acoustic camera is superior device because it can provide images with more accurate details than what the optical cameras provide even in turbid water. However, the structure of the acoustic image is very dissimilar to that of optical image because the acoustic camera provides sequences of distance and azimuth angle data, which elevation angle data are missing. Consequently, it is difficult to generate simulated acoustic images although simulating realistic images is vital for verification for performance of proposed algorithms and saving costs and time. This paper describes the principles of acoustic natures and imaging geometry model of the acoustic camera which greatly contribute to developing simulator for the acoustic images. When compared to a state-of-the-art technique, our approach demonstrates surpassing performance in representing simulated acoustic images in complete gray-scale. We evaluate the simulated images from developed simulator by comparing with real acoustic images. The results indicate that the proposed simulator can generate realistic virtual acoustic images based on the principles.},
  keywords = {acoustic camera,acoustic image,acoustic imaging simulator,Acoustic waves,Attenuation,Azimuth,Cameras,Gray-scale}
}

@article{lehoang2020_DeepGabo,
  title = {Deep {{Gabor Neural Network}} for {{Automatic Detection}} of {{Mine-Like Objects}} in {{Sonar Imagery}}},
  author = {Le Hoang, Thanh and Phung, Son and Chapple, Philip and Bouzerdoum, Abdesselam and Ritz, Christian and Tran, Le Chung},
  year = {2020},
  month = may,
  journal = {IEEE Access},
  volume = {PP},
  pages = {1--1},
  doi = {10/gpb94t},
  abstract = {With the advances in sonar imaging technology, sonar imagery has increasingly been used for oceanographic studies in civilian and military applications. High-resolution imaging sonars can be mounted on various survey platforms, typically autonomous underwater vehicles, which provide enhanced speed and improved data quality with long-range support. This paper addresses the automatic detection of mine-like objects using sonar images. The proposed Gabor-based detector is designed as a feature pyramid network with a small number of trainable weights. Our approach combines both semantically weak and strong features to handle mine-like objects at multiple scales effectively. For feature extraction, we introduce a parameterized Gabor layer which improves the generalization capability and computational efficiency. The steerable Gabor filtering modules are embedded within the cascaded layers to enhance the scale and orientation decomposition of images. The entire deep Gabor neural network is trained in an end-to-end manner from input sonar images with annotated mine-like objects. An extensive experimental evaluation on a real sonar dataset shows that the proposed method achieves competitive performance compared to the existing approaches.}
}

@article{luczynski2021_UndeInsp,
  title = {Underwater Inspection and Intervention Dataset},
  author = {Luczynski, Tomasz and Willners, Jonatan Scharff and Vargas, Elizabeth and Roe, Joshua and Xu, Shida and Cao, Yu and Petillot, Yvan and Wang, Sen},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.13628 [cs]},
  eprint = {2107.13628},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.13628},
  urldate = {2022-02-03},
  abstract = {This paper presents a novel dataset for the development of visual navigation and simultaneous localisation and mapping (SLAM) algorithms as well as for underwater intervention tasks. It differs from existing datasets as it contains ground truth for the vehicle's position captured by an underwater motion tracking system. The dataset contains distortion-free and rectified stereo images along with the calibration parameters of the stereo camera setup. Furthermore, the experiments were performed and recorded in a controlled environment, where current and waves could be generated allowing the dataset to cover a wide range of conditions - from calm water to waves and currents of significant strength.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition}
}

@article{mackenzie1981_NineEqua,
  title = {Nine-Term Equation for Sound Speed in the Oceans},
  author = {Mackenzie, Kenneth V.},
  year = {1981},
  journal = {The Journal of the Acoustical Society of America},
  volume = {70},
  number = {3},
  pages = {807--812},
  publisher = {{Acoustical Society of America}},
  doi = {10/d56265}
}

@article{mallios2017_UndeCave,
  title = {Underwater Caves Sonar Data Set},
  author = {Mallios, Angelos and Vidal, Eduard and Campos, Ricard and Carreras, Marc},
  year = {2017},
  month = oct,
  journal = {The International Journal of Robotics Research},
  volume = {36},
  number = {12},
  pages = {1247--1251},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10/gcjf9m},
  url = {https://doi.org/10.1177/0278364917732838},
  urldate = {2022-01-31},
  abstract = {This paper describes a data set collected with an autonomous underwater vehicle testbed in the unstructured environment of an underwater cave complex. The vehicle is equipped with two mechanically scanned imaging sonar sensors to simultaneously map the caves horizontal and vertical surfaces, a Doppler velocity log, two inertial measurement units, a depth sensor, and a vertically mounted camera imaging the sea floor for ground truth validation at specific points. The testbed collected the data in July 2013, guided by a human diver, to sidestep autonomous navigation in a complex environment. For ease of use, the original robot operating system bag files are provided together with a version combining imagery and human-readable text files for processing on other environments.}
}

@inproceedings{manhaes2016_UUVSimu,
  title = {{{UUV Simulator}}: {{A Gazebo-based}} Package for Underwater Intervention and Multi-Robot Simulation},
  shorttitle = {{{UUV Simulator}}},
  booktitle = {{{OCEANS}} 2016 {{MTS}}/{{IEEE Monterey}}},
  author = {Manh{\~a}es, Musa Morena Marcusso and Scherer, Sebastian A. and Voss, Martin and Douat, Luiz Ricardo and Rauschenbach, Thomas},
  year = {2016},
  month = sep,
  pages = {1--8},
  doi = {10/gf7nz9},
  abstract = {This paper describes the Unmanned Underwater Vehicle (UUV) Simulator, an extension of the open-source robotics simulator Gazebo to underwater scenarios, that can simulate multiple underwater robots and intervention tasks using robotic manipulators. This is achieved mainly through a set of newly implemented plugins that model underwater hydrostatic and hydrodynamic effects, thrusters, sensors, and external disturbances. In contrast to existing solutions, it reuses and extends a general-purpose robotics simulation platform to underwater environments.},
  keywords = {Dynamics,Load modeling,Robot sensing systems,Vehicle dynamics,Vehicles}
}

@misc{medwin1999_FundAcou,
  title = {Fundamentals of Acoustical Oceanography},
  author = {Medwin, Herman and Clay, Clarence S. and Stanton, Timothy K.},
  year = {1999},
  publisher = {{Acoustical Society of America}}
}

@misc{mogasoftwareSeaVMOSA,
  title = {{{SeaView MOSAIC}}},
  author = {{Moga Software}},
  url = {https://www.mogasw.com/seaview/mosaic/},
  urldate = {2022-01-31}
}

@article{munk1998_AbysReci,
  title = {Abyssal Recipes {{II}}: {{Energetics}} of Tidal and Wind Mixing},
  shorttitle = {Abyssal Recipes {{II}}},
  author = {Munk, Walter and Wunsch, Carl},
  year = {1998},
  journal = {Deep Sea Research Part I: Oceanographic Research Papers},
  volume = {45},
  number = {12},
  pages = {1977--2010},
  publisher = {{Elsevier}},
  doi = {10/bt7s3f}
}

@misc{navyrecognition2020_SolsMult,
  title = {Solstice Multi-Aperture Sonar Has Been Integrated into Mine Warfare Command Software Used by {{Australian Navy}}},
  author = {{Navy Recognition}},
  year = {2020},
  journal = {Navy Recognition},
  url = {https://www.navyrecognition.com/index.php/naval-news/naval-news-archive/2020/september/8992-solstice-multi-aperture-sonar-has-been-integrated-into-mine-warfare-command-software-used-australian-navy.html},
  urldate = {2022-02-03},
  abstract = {Sonardyne has announced that its Solstice multi-aperture sonar has been integrated into mine warfare command software used by the Royal Australian Navy (RAN).},
  langid = {british}
}

@article{newman2008_MOOSMiss,
  title = {{{MOOS}} - {{Mission Orientated Operating Suite}}},
  author = {Newman, P.},
  year = {2008},
  journal = {undefined},
  url = {https://www.semanticscholar.org/paper/MOOS-Mission-Orientated-Operating-Suite-Newman/5abb6be8b008d898f6aec4d76f3eefb26a905c7b},
  urldate = {2022-01-31},
  abstract = {The underlying philosophy of MOOS and the resulting perceived benefits are described and a set of high level descriptions of principal mission-oriented MOOS processes constitute a resilient, distributed and coordinated suite of software suitable for in-the-field deployment of sub-sea and land research robots. This paper is about simple to use, extensible software for mobile robotic research. It is concerned with a project called MOOS \textendash{} an acronym for Mission Oriented Operating Suite. MOOS is an umbrella term for a set of libraries and applications designed to facilitate research in the mobile robotic domain. The spectrum of functionality provided ranges over low-level, multi-platform communications, dynamic control, high precision navigation and path planning, concurrent mission task arbitration and execution, mission logging and playback. The first part of the paper describes the underlying philosophy of MOOS and the resulting perceived benefits. The work then moves on to describe the details of the design and implementations of core system components. There then follows a set of high level descriptions of principal mission-oriented MOOS processes. Collectively these processes constitute a resilient, distributed and coordinated suite of software suitable for in-the-field deployment of sub-sea and land research robots.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{nikolov1997_Inte3D,
  title = {Interactive 3-{{D Visualisation}} of {{Sonar Data Using VRML}}},
  author = {Nikolov, S. and Loke, E. and {du Buf}, J.},
  year = {1997},
  month = sep,
  abstract = {In this paper we describe a new software system for interactive three-dimensional visualisation of sonar data. Novel approaches to 3-D volumetric sonar data reconstruction and segmentation are presented. VRMLworlds are generated from the segmented data with the aim of quick and interactive 3-D visualisation and interpretation of the seafloor. VRML simulations of 3-D sonar data are constructed to visually compare the results of different segmentation algorithms. 1. INTRODUCTION New visualisation techniques are playing a crucial role in the current efforts to better understand the ocean environment. Such techniques have emerged in the process of integration of image processing, computer vision, and computer graphics methods. Filtering, reconstruction, registration, and segmentation of underwater data aid us in generating images which increase our comprehension of the complex structure of the seafloor. The new visualisation techniques presented in this paper are being developed as part ...},
  keywords = {⛔ No DOI found}
}

@misc{noaaExplTool,
  title = {Exploration {{Tools}}: {{Synthetic Aperture Sonar}}: {{NOAA Office}} of {{Ocean Exploration}} and {{Research}}},
  shorttitle = {Exploration {{Tools}}},
  author = {{NOAA}},
  url = {https://oceanexplorer.noaa.gov/technology/sonar/sas.html},
  urldate = {2022-02-03},
  langid = {american}
}

@misc{noaaphotolibrary2016_ArtiConc,
  title = {Artist's Conception of Multibeam Sonar on {{NOAA Ship NANCY FOSTER}}},
  author = {{NOAA Photo Library}},
  year = {2016},
  month = jul,
  url = {https://www.flickr.com/photos/noaaphotolib/27555144884/},
  urldate = {2022-01-28},
  abstract = {Artist's conception of multibeam sonar on NOAA Ship NANCY FOSTER     Image ID: fis01334, NOAA's Fisheries Collection     Credit: NOS/NCCOS/CCMA},
  copyright = {Attribution License}
}

@misc{obrien2018_Geos,
  title = {Geosvy/Lstjsf},
  author = {O'Brien, Thomas},
  year = {2018},
  month = dec,
  url = {https://github.com/Geosvy/lstjsf},
  urldate = {2022-01-31},
  abstract = {List selected values of an Edgetech .jsf file}
}

@misc{obrien2019_Geos,
  title = {Geosvy/Jsfmesgtype},
  author = {O'Brien, Thomas},
  year = {2019},
  month = oct,
  url = {https://github.com/Geosvy/jsfmesgtype},
  urldate = {2022-01-31},
  abstract = {Parse and list the message types of an Edgetech sonar .jsf file.}
}

@incollection{ribas2010_UndeMech,
  title = {Understanding Mechanically Scanned Imaging Sonars},
  booktitle = {Underwater {{SLAM}} for {{Structured Environments Using}} an {{Imaging Sonar}}},
  author = {Ribas, David and Ridao, Pere and Neira, Jos{\'e}},
  year = {2010},
  pages = {37--46},
  publisher = {{Springer}}
}

@article{savini2011_SideSona,
  title = {Side-Scan Sonar as a Tool for Seafloor Imagery: {{Examples}} from the {{Mediterranean Continental Margin}}},
  shorttitle = {Side-Scan Sonar as a Tool for Seafloor Imagery},
  author = {Savini, Alessandra},
  year = {2011},
  journal = {Sonar Systems},
  publisher = {{IntechOpen}},
  doi = {10/gpb94d}
}

@phdthesis{sena2018_ShalWate,
  title = {Shallow Water Remote Sensing Using Sonar Improved with Geostatistics and Stochastic Resonance Data Processing},
  author = {Sena, Andre Luis Sousa},
  year = {2018},
  school = {Universitat de les Illes Balears}
}

@inproceedings{sheffer2018_GeomCorr,
  title = {Geometrical {{Correction}} of {{Side-scan Sonar Images}}},
  booktitle = {2018 {{IEEE International Conference}} on the {{Science}} of {{Electrical Engineering}} in {{Israel}} ({{ICSEE}})},
  author = {Sheffer, Tal and Guterman, Hugo},
  year = {2018},
  pages = {1--5},
  publisher = {{IEEE}},
  doi = {10/gpb94p}
}

@misc{SideSona,
  title = {Sidescan {{Sonar Data Formats}}},
  url = {https://chesapeaketech.com/wp-content/uploads/docs/SonarWiz7_UG/HTML/cheasapeaketech.com/sidescan_sonar_data_formats.htm},
  urldate = {2022-01-31},
  abstract = {Sidescan Sonar Data Formats  SonarWiz supports import of sidescan data in the following file formats:~~~~~~~     Manufacturer / Format Name  Format Extension  C-MAX  XTF, CM2  CODA  COD  Edgetech  JSF  eXtended Triton Format   XTF  Geoacoustics  GCF,},
  langid = {english}
}

@misc{smith1996_ShipTrac,
  title = {Ship {{Tracks}}},
  author = {Smith, W. H. F. and Sandwell, D. T.},
  year = {1996},
  publisher = {{Version}}
}

@misc{sonardyne2017_MASVs,
  title = {{{MAS}} vs {{SAS}} - {{When}} Should We Use Multiple Aperture Sonar?},
  author = {{Sonardyne}},
  year = {2017},
  month = dec,
  journal = {Sonardyne},
  url = {https://www.sonardyne.com/mas-vs-sas-when-should-we-use-multiple-aperture-sonar/},
  urldate = {2022-02-03},
  abstract = {Solstice is a Multi Aperture Sonar which functions like a top performing traditional side-scan sonar, but processed very differently.},
  langid = {british}
}

@misc{sonarwizEdgeJSF,
  title = {{{EdgeTech JSF Import Options}}},
  author = {{SonarWiz}},
  url = {https://chesapeaketech.com/wp-content/uploads/docs/SonarWiz7_UG/HTML/cheasapeaketech.com/edgetech_jsf_import_options.htm},
  urldate = {2022-01-31},
  abstract = {EdgeTech JSF Import Options    The EdgeTech JSF Import Options are found on the EdgeTech JSF tab of the File Type Specific Options window.      Signal Component    JSF stores sonar samples at complex numbers. So, one can choose the real, imaginary or},
  langid = {english}
}

@misc{sonarwizSonaSide,
  title = {{{SonarWiz}} - {{Sidescan Sonar Mapping}} \& {{Post-Processing}} | {{Chesapeake Technology}}},
  author = {{SonarWiz}},
  url = {https://chesapeaketech.com/products/sonarwiz-sidescan/},
  urldate = {2022-01-31}
}

@misc{SounMetr,
  title = {Sound {{Metrics Corp}}},
  url = {http://www.soundmetrics.com/},
  urldate = {2022-01-28}
}

@article{sternlicht2017_HistDeve,
  title = {Historical Development of Side Scan Sonar},
  author = {Sternlicht, Daniel D.},
  year = {2017},
  journal = {The Journal of the Acoustical Society of America},
  volume = {141},
  number = {5},
  pages = {4041--4041},
  publisher = {{Acoustical Society of America}},
  doi = {10/gpb94c}
}

@incollection{thurman2009_MultInte,
  title = {Multi-{{Sonar Integration}} and the {{Advent}} of {{Senor Intelligence}}},
  author = {Thurman, Edward and Riordan, James and Toal, Daniel},
  year = {2009},
  month = feb,
  doi = {10.5772/39412},
  abstract = {Increased interest in the detailed exploration of our ocean seabeds has spurred the development and technological advancements in sonar technology. Sonar is an essential part of a modern marine survey system and has been successfully employed to record the composition, physical attributes, and habitat and community patterns of the seafloor. The integration of multiple sonar sensors into a marine survey suite allows for the simultaneous collection of individual datasets of the same seafloor region. A move towards multi-sensor integration is becoming more and more apparent in the marine industry, allowing for the enhancement of decision making and data analysis by exploiting the synergy in the information acquired from multiple sources. However, the integration and concurrent operation of multiple sonar sensors in a marine survey suite creates issues of cross-sensor acoustic interference. The contamination caused by sensor crosstalk severely degrades the resulting datasets, and hence, the data examination and understanding. Traditionally, compromises were sought to avoid this sensor crosstalk by mobilising separate surveys for each of the interfering sensors or by separating the operating frequency of the sonar sufficiently far that they are undetectable from one another, and more recently, in particular in the operation of UUV platforms, survey sensor control routines have been developed. Nevertheless, solutions to the problem of sensor crosstalk remain inadequate and inefficient. The intelligence of sensors is advancing rapidly. Innovative developments in sensor technology have enabled the data acquisition, processing and decision making to occur in real-time during survey operations. An approach to the real-time adaptive control of multiple high-frequency sonar systems was presented in this chapter. This approach is based around a centralised sensor payload controller which manages the integrated sensors during survey missions, facilitating the operation of co-located, high-frequency sonar. The multibeam is the master system and supplies the raw data to be processed in real-time to provide a priori bathymetry data to auxiliary acoustic sensors. The automated system is based on the interleaving of the sonar transmission-reception cycles in a non-interfering fashion. By allowing real-time decision making to be made based on real-time mission data, the system optimises the execution of the seabed mapping survey and improves the quality of the resulting data, thereby significantly increasing survey productivity, and consequently, the data analysis and interpretation.},
  isbn = {978-3-902613-48-6}
}

@techreport{usgs2018_HydrSurv,
  type = {Fact {{Sheet}}},
  title = {Hydrographic {{Surveys}} of {{Rivers}} and {{Lakes Using}} a {{Multibeam Echosounder Mapping System}}},
  author = {{USGS}},
  year = {2018},
  series = {Fact {{Sheet}}},
  langid = {english}
}

@article{vanveen1988_BeamVers,
  title = {Beamforming: {{A}} Versatile Approach to Spatial Filtering},
  shorttitle = {Beamforming},
  author = {Van Veen, Barry D. and Buckley, Kevin M.},
  year = {1988},
  journal = {IEEE assp magazine},
  volume = {5},
  number = {2},
  pages = {4--24},
  publisher = {{IEEE}},
  doi = {10/fv5wnm}
}

@misc{waterlooaquadrone2021_Aqua2020,
  title = {Aquadrone 2020},
  author = {{Waterloo Aquadrone}},
  year = {2021},
  month = dec,
  url = {https://github.com/Waterloo-Aquadrone/aquadrone-core},
  urldate = {2022-02-03},
  howpublished = {Waterloo Aquadrone}
}


