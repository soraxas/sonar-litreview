
@article{hahner2019_SemaUnde,
  title = {Semantic {{Understanding}} of {{Foggy Scenes}} with {{Purely Synthetic Data}}},
  author = {Hahner, Martin and Dai, Dengxin and Sakaridis, Christos and Zaech, Jan-Nico and Van Gool, Luc},
  year = {2019},
  month = oct,
  journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  eprint = {1910.03997},
  eprinttype = {arxiv},
  pages = {3675--3681},
  doi = {10/gn6d93},
  url = {http://arxiv.org/abs/1910.03997},
  urldate = {2022-01-18},
  abstract = {This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen real-world foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{johnson-roberson2017_DrivMatr,
  title = {Driving in the {{Matrix}}: {{Can Virtual Worlds Replace Human-Generated Annotations}} for {{Real World Tasks}}?},
  shorttitle = {Driving in the {{Matrix}}},
  author = {{Johnson-Roberson}, Matthew and Barto, Charles and Mehta, Rounak and Sridhar, Sharath Nittur and Rosaen, Karl and Vasudevan, Ram},
  year = {2017},
  month = feb,
  journal = {arXiv:1610.01983 [cs]},
  eprint = {1610.01983},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1610.01983},
  urldate = {2022-01-18},
  abstract = {Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have relied upon massive amounts of human annotated training data. This time consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for the training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor-based classification problems like those that appear in self-driving cars. The source code and data to train and validate the networks described in this paper are made available for researchers.},
  archiveprefix = {arXiv},
  keywords = {â›” No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}


